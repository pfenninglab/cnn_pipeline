# Project

project:
  desc: Name of `wandb` project for tracking.
  value: default-project

entity:
  desc: Name of `wandb` account for tracking.
  value: cmu-cbd-pfenninglab

# WandB logging
dir:
  desc: Directory to create the wandb folder to save model checkpoints and logs.
  value: 
name:
  desc: Short descriptive name of this run to show on the W&B.com logger.
  value: 

# Data

train_data_paths:
  desc: Paths to FASTA, BED, or NarrowPeak files for training data.
  # fasta or bed format:
  #   fasta:
  #     - negatives.fa
  #     - positives.fa
  #   bed:
  #     - genome: genome.fa
  #       intervals: negatives.bed
  #     - genome: genome.fa
  #       intervals: positives.bed
  value:
    - genome: /projects/pfenninggroup/machineLearningForComputationalBiology/halLiftover_chains/data/raw_data/fasta/Mus_musculus.fa
      intervals: /projects/pfenninggroup/mouseCxStr/NeuronSubtypeATAC/Zoonomia_CNN/mouse_SST/FinalModelData/mouse_SST_neg_TRAIN.bed
    - genome: /projects/pfenninggroup/machineLearningForComputationalBiology/halLiftover_chains/data/raw_data/fasta/Mus_musculus.fa
      intervals: /projects/pfenninggroup/mouseCxStr/NeuronSubtypeATAC/Zoonomia_CNN/mouse_SST/FinalModelData/mouse_SST_pos_TRAIN.bed

train_targets:
  desc: Targets corresponding to each training data path.
  # To use a constant value per file, e.g. 0 or "neg", just put the value here.
  # To extract the value from each line of a BED or NarrowPeak file, put:
  #     column: <column number>
  value:
    - 0
    - 1

val_data_paths:
  desc: Paths to FASTA, BED, or NarrowPeak files for validation data.
  value:
    - genome: /projects/pfenninggroup/machineLearningForComputationalBiology/halLiftover_chains/data/raw_data/fasta/Mus_musculus.fa
      intervals: /projects/pfenninggroup/mouseCxStr/NeuronSubtypeATAC/Zoonomia_CNN/mouse_SST/FinalModelData/mouse_SST_neg_VAL.bed
    - genome: /projects/pfenninggroup/machineLearningForComputationalBiology/halLiftover_chains/data/raw_data/fasta/Mus_musculus.fa
      intervals: /projects/pfenninggroup/mouseCxStr/NeuronSubtypeATAC/Zoonomia_CNN/mouse_SST/FinalModelData/mouse_SST_pos_VAL.bed

val_targets:
  desc: Targets corresponding to each validation data path.
  value:
    - 0
    - 1

targets_are_classes:
  desc: Whether to treat targets as a set of discrete classes (classification) or as continuous values (regression).
  value: true

additional_val_data_paths:
  desc: Paths to additional validation datasets. Metrics will be reported for each set separately.
  # fasta or bed format:
  #   fasta:
  #     - [set1_negatives.fa, set1_positives.fa]
  #     - [set2_negatives.fa, set2_positives.fa]
  #   bed:
  #     - [{genome: genome.fa, intervals: set1_negatives.bed},
  #        {genome: genome.fa, intervals: set1_positives.bed}
  #     - [{genome: genome.fa, intervals: set2_negatives.bed},
  #        {genome: genome.fa, intervals: set2_positives.bed}
  #
  # it is fine to include a dataset with only negative or only positive examples, e.g.
  # - [set1_negatives.fa]
  value:
    - [/projects/pfenninggroup/mouseCxStr/NeuronSubtypeATAC/Zoonomia_CNN/evaluations/SST/Eval1_mm10_VAL.fa]
    - [/projects/pfenninggroup/mouseCxStr/NeuronSubtypeATAC/Zoonomia_CNN/evaluations/SST/Eval2_hg38_VAL.fa]
    - [/projects/pfenninggroup/mouseCxStr/NeuronSubtypeATAC/Zoonomia_CNN/evaluations/SST/Eval3_hg38_VAL.fa]
    - [/projects/pfenninggroup/mouseCxStr/NeuronSubtypeATAC/Zoonomia_CNN/evaluations/SST/Eval4_mm10_VAL.fa]
    - [/projects/pfenninggroup/mouseCxStr/NeuronSubtypeATAC/Zoonomia_CNN/evaluations/SST/Eval5_mm10_VAL.fa]
    - [/projects/pfenninggroup/mouseCxStr/NeuronSubtypeATAC/Zoonomia_CNN/evaluations/SST/Eval6_mm10_VAL.fa]
    - [/projects/pfenninggroup/mouseCxStr/NeuronSubtypeATAC/Zoonomia_CNN/evaluations/SST/Eval7_mm10_VAL.fa]
    - [/projects/pfenninggroup/mouseCxStr/NeuronSubtypeATAC/Zoonomia_CNN/evaluations/SST/Eval8_mm10_VAL.fa]
    - [/projects/pfenninggroup/mouseCxStr/NeuronSubtypeATAC/Zoonomia_CNN/evaluations/SST/Eval9_mm10_VAL.fa]
    - [/projects/pfenninggroup/mouseCxStr/NeuronSubtypeATAC/Zoonomia_CNN/evaluations/SST/Eval10_mm10_VAL.fa]

additional_val_targets:
  desc: Targets corresponding to additional validation paths.
  value:
    - [1]
    - [0]
    - [1]
    - [0]
    - [1]
    - [1]
    - [0]
    - [1]
    - [1]
    - [0]

# Training

model_checkpoint:
  desc: Path to saved model file (.h5). Use this if you want to resume training from this checkpoint.
  value: none

batch_size:
  desc: Batch size for training and validation.
  value: 512

num_epochs:
  desc: Number of training epochs.
  value: 25

metric_pos_label:
  desc: Positive label to use for binary metrics.
  value: 1

early_stopping_callbacks:
  desc: Callbacks to use for early stopping. Use arguments as in tf.keras.callbacks.EarlyStopping.
  value:
    - monitor: val_auprc
      patience: 5
      verbose: 1
      mode: max

use_exact_val_metrics:
  desc: If true, use exact validation metrics during training (requires loading whole validation set into RAM). If false, use a close approximation (+/- ~2%) that streams the validation set without loading into RAM.
  value: true

use_reverse_complement:
  desc: If true, add reverse complement sequences to the training set, doubling the training set size.
  value: true

class_weight:
  desc: Scheme to weight the loss function according to the class.
  allowed_values: ['none', 'reciprocal', 'proportional']
  value: none

loss_function:
  desc: Loss function to use during training.
  # classification: use 'sparse_categorical_crossentropy'
  # regression: use 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', or 'huber'
  allowed_values: ['sparse_categorical_crossentropy', 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'huber']
  value: sparse_categorical_crossentropy

# Optimization

optimizer:
  desc: Weight optimization algorithm to use.
  allowed_values: ['sgd', 'adam']
  value: adam

optimizer_args:
  desc: Arguments for optimizer.
  # Default values for adam:
  #   beta_1: 0.9
  #   beta_2: 0.999
  # Default values for sgd:
  #   momentum: 0.9
  value:
    beta_1: 0.9
    beta_2: 0.999

lr_schedule:
  desc: Learning rate schedule to use.
  allowed_values: ['cyclic', 'exponential']
  value: exponential

lr_cyc_scale_fn:
  desc: Scaling function for each cycle of cyclic LR schedule.
  allowed_values: ['triangular', 'triangular2']
  value: triangular2

lr_init:
  desc: Initial learning rate.
  value: 4.e-4

lr_max:
  desc: Maximum learning rate for cyclic LR schedule.
  value: 1.e-2

lr_exp_decay_per_epoch:
  desc: Float in [0, 1]. Decay per epoch, for exponential LR schedule.
  value: 0.95

lr_cyc_num_cycles:
  desc: Float > 0. Number of cycles, for cyclic LR schedule.
  value: 1.0

clr_tail_epochs:
  desc: Int >= 0. Number of epochs to fine-tune model following cyclic LR training. Learning rate will decrease linearly from lr_init to lr_init / 10.
  value: 5

momentum_schedule:
  desc: Momentum schedule to use.
  allowed_values: ['constant', 'cyclic']
  value: constant

momentum_base:
  desc: Minimum momentum for cyclic momentum schedule.
  value: 0.875

momentum_max:
  desc: Maximum momentum for cyclic momentum schedule.
  value: 0.99

# Initialization

kernel_initializer:
  desc: Initializer for kernel weights.
  # `identifier` can be any Keras initializer string. See https://keras.io/api/layers/initializers/
  # `args` is an optional mapping of keyword arguments and values for this identifier, e.g.
  # identifier: he_normal
  # args:
  #   seed: 0
  value:
    identifier: glorot_uniform

bias_initializer:
  desc: Initializer for bias weights.
  value:
    identifier: zeros

# Regularization

l2_reg_conv:
  desc: Strength of L2 regularization for each convolutional layer.
  value: 1.e-4

l2_reg_dense:
  desc: Strength of L2 regularization for each dense layer.
  value: 1.e-4

l2_reg_final:
  desc: Strength of L2 regularization for the final (output) dense layer.
  value: 1.e-4

dropout_rate_conv:
  desc: Float in [0, 1]. Rate for dropout after each convolutional layer.
  value: 0.3

dropout_rate_dense:
  desc: Float in [0, 1]. Rate for dropout after each dense layer.
  value: 0.3

# Architecture

num_conv_layers:
  desc: Number of 1D convolutional layers. Each is followed by a dropout layer.
  value: 2

conv_filters:
  desc: Number of filters in each convolutional layer.
  value: 500

conv_width:
  desc: Width of 1D convolutional kernels.
  value: 11

conv_stride:
  desc: Stride of convolutional layers.
  value: 1

max_pool_size:
  desc: Width of max pooling partitions.
  value: 26

max_pool_stride:
  desc: Stride of max pooling layer.
  value: 26

num_dense_layers:
  desc: Number of dense layers, not including the final layer. Each is followed by a dropout layer.
  value: 1

dense_filters:
  desc: Number of filters in dense layer.
  value: 300

# Interpretation

interp_model_path:
  desc: Path to model to interpret.
  value: /home/csestili/models/model-5layer.h5

shap_num_bg:
  desc: Number of background samples to use for SHAP model interpretation. Taken from training set.
  value: 20

shap_num_fg:
  desc: Number of foreground samples to use for SHAP model interpretation. Taken from positive subset of validation set.
  value: 5

shap_pos_label:
  desc: Positive label to use for SHAP model interpretation.
  value: 1

modisco_normalization:
  desc: Normalization method to use for importance scores.
  allowed_values: ['none', 'pointwise', 'gkm_explain']
  value: gkm_explain
